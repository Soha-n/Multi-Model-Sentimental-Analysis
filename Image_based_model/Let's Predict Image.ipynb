{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac0b454-2e91-487d-a87e-78655e97130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94102b-d796-4cf0-82cc-dec9a7742371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61aceed-cc5c-4f7b-9f7d-dd659e26743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "class ImagePredictor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        img = image.load_img(image_path, target_size=(299, 299))\n",
    "        \n",
    "        img_array = image.img_to_array(img)\n",
    "        \n",
    "        img_array = img_array / 255.0\n",
    "        \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        return img_array\n",
    "\n",
    "    def image_classify(self, image_path):\n",
    "        \n",
    "        processed_image = self.preprocess_image(image_path)\n",
    "\n",
    "        predictions = self.model.predict(processed_image)\n",
    "\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "        return str(predicted_class[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20aac004-2e5e-4060-bb75-5efeec265ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 0): Anger\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'image_model' is your trained model (e.g., 'multimodal_sentiment_model_67.h5')\n",
    "image_predictor = ImagePredictor(model=image_model)\n",
    "\n",
    "# Example usage\n",
    "image_path = '6 Emotions for image classification/anger.jpg/g913ioe8wwg2ej06h0.jpg'  # Replace with the path to your image\n",
    "predicted_class = image_predictor.image_classify(image_path)\n",
    "\n",
    "predicted_index = int(predicted_class)\n",
    "\n",
    "# Access the corresponding emotion label using the index\n",
    "emotion_labels = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "\n",
    "if 0 <= predicted_index < len(emotion_labels):\n",
    "    corresponding_emotion = emotion_labels[predicted_index]\n",
    "    print(f\"Predicted emotion (index {predicted_index}): {corresponding_emotion}\")\n",
    "else:\n",
    "    print(f\"Index {predicted_index} is out of range for emotion_labels.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4fc76-3655-4602-aea3-758257fa2f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6bf67-01aa-40e8-9037-349090f3263b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a6a64-3e37-45e1-a9c6-5f4adb0dacc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc88500-6afa-42f1-a526-ef3c62d6aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Error: '0' is not a recognized emotion label.\n",
      "Index -1 is out of range for emotion_labels.\n"
     ]
    }
   ],
   "source": [
    "image_predictor = ImagePredictor(model=image_model)\n",
    "\n",
    "image_path = '6 Emotions for image classification/anger.jpg/g913ioe8wwg2ej06h0.jpg'\n",
    "predicted_class = image_predictor.image_classify(image_path)\n",
    "\n",
    "# Check the data type of predicted_class\n",
    "if isinstance(predicted_class, int):\n",
    "    # Integer class index\n",
    "    predicted_index = predicted_class\n",
    "elif isinstance(predicted_class, str):\n",
    "    # String class label\n",
    "    try:\n",
    "        predicted_index = emotion_labels.index(predicted_class)\n",
    "    except ValueError:\n",
    "        print(f\"Error: '{predicted_class}' is not a recognized emotion label.\")\n",
    "        predicted_index = -1  # Or assign a default value\n",
    "elif isinstance(predicted_class, list):\n",
    "    # Probability distribution\n",
    "    predicted_index = np.argmax(predicted_class)\n",
    "else:\n",
    "    raise ValueError(\"Unexpected data type for predicted_class\")\n",
    "\n",
    "# Access the corresponding emotion label using the index\n",
    "if 0 <= predicted_index < len(emotion_labels):\n",
    "    corresponding_emotion = emotion_labels[predicted_index]\n",
    "    print(f\"Predicted emotion (index {predicted_index}): {corresponding_emotion}\")\n",
    "else:\n",
    "    print(f\"Index {predicted_index} is out of range for emotion_labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7a4fba-f397-4e72-b9c1-6ed2cacd3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    " emotion_labels = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b720f3ca-914e-4563-a62a-628730110664",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_predictor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(image_predictor, f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "with open('image_predictor.pkl', 'wb') as f:\n",
    "    pickle.dump(image_predictor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba254bfb-eab1-4a22-be71-b4f579adf6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfcd161b-59da-4da9-be49-45268c1fb1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess and predict sentiment from an image\n",
    "def predict_image_sentiment(image_path):\n",
    "    # Load the image with the target size that your model expects\n",
    "    img = image.load_img(image_path, target_size=(299, 299))  # Resize image to (299, 299)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Expand dimensions to match the model's input format (batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Normalize the image as the model expects (ImageNet preprocessing for Xception)\n",
    "    img_array = img_array / 255.0  # Rescaling\n",
    "    \n",
    "    # Predict sentiment with the trained model\n",
    "    prediction = video_model.predict(img_array)\n",
    "    \n",
    "    # Define your emotion classes\n",
    "    emotions = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "\n",
    "    # Get the predicted class (index of max value)\n",
    "    predicted_class = np.argmax(prediction, axis=-1)\n",
    "    \n",
    "    # Map the predicted class to the corresponding emotion\n",
    "    predicted_emotion = emotions[predicted_class[0]]\n",
    "    \n",
    "    return predicted_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "176bc014-5687-4464-8c14-9d2cff38f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Predicted emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_path = '6 Emotions for image classification/anger.jpg/g913ioe8wwg2ej06h0.jpg'  # Replace with the path to your image\n",
    "predicted_emotion = predict_image_sentiment(image_path)\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2338f-655f-4b9d-84cf-58ca63cc50bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119ee540-8d2b-4203-956f-0effd0bd90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '6 Emotions for image classification/anger.jpg/g913ioe8wwg2ej06h0.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1bd86c-23cd-4c7b-90a1-f8ef8faf3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "image_model = load_model('multimodal_sentiment_model_67.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5331da6-2fc6-4f4e-ba52-bc5e02313d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"xception\" is incompatible with the layer: expected shape=(None, 299, 299, 3), found shape=(1, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     30\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6 Emotions for image classification/anger.jpg/g913ioe8wwg2ej06h0.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the path to the image you want to predict\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m predicted_emotion \u001b[38;5;241m=\u001b[39m predict_image_sentiment(image_path)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted emotion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_emotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m, in \u001b[0;36mpredict_image_sentiment\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m img_array \u001b[38;5;241m=\u001b[39m preprocess_input(img_array)  \u001b[38;5;66;03m# Example for VGG16, modify for your model if needed\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m prediction \u001b[38;5;241m=\u001b[39m image_model\u001b[38;5;241m.\u001b[39mpredict(img_array)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Map the prediction to your emotion labels\u001b[39;00m\n\u001b[0;32m     24\u001b[0m emotions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisgust\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHappiness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSadness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurprise\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"xception\" is incompatible with the layer: expected shape=(None, 299, 299, 3), found shape=(1, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input  # or use your model's preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your model\n",
    "image_model = load_model('multimodal_sentiment_model_67.h5')\n",
    "\n",
    "# Function to preprocess and predict sentiment from image\n",
    "def predict_image_sentiment(image_path):\n",
    "    # Load the image\n",
    "    img = image.load_img(image_path, target_size=(224, 224))  # Adjust target_size as per your model's input shape\n",
    "    img_array = image.img_to_array(img)  # Convert image to array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    \n",
    "    # Preprocess the image as needed (e.g., normalization)\n",
    "    img_array = preprocess_input(img_array)  # Example for VGG16, modify for your model if needed\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = image_model.predict(img_array)\n",
    "    \n",
    "    # Map the prediction to your emotion labels\n",
    "    emotions = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "    predicted_emotion = emotions[np.argmax(prediction)]\n",
    "    \n",
    "    return predicted_emotion\n",
    "\n",
    "# Example usage\n",
    "image_path = '6 Emotions for image classification/anger.jpg/g913ioe8wwg2ej06h0.jpg'  # Replace with the path to the image you want to predict\n",
    "predicted_emotion = predict_image_sentiment(image_path)\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c172bf-c472-42d8-8c85-27120263373e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
