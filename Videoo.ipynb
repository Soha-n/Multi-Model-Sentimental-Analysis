{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72be36e-cf82-4728-9fe3-07154aca2a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssttg\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from predictor_all import AudioPredictor\n",
    "from predictor_all import VoteClassifier\n",
    "from predictor_all import ImagePredictor\n",
    "from keras.models import load_model\n",
    "import speech_recognition as sr\n",
    "from collections import Counter\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import threading\n",
    "import speech_recognition as sr\n",
    "from collections import Counter\n",
    "from keras.preprocessing import image\n",
    "import librosa\n",
    "from moviepy.editor import VideoFileClip\n",
    "import tempfile\n",
    "import os\n",
    "from queue import Queue\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import image\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e02033d-5e3a-4002-8437-ce7a326287eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30c8c0f-7099-48d2-9430-e7275fa4c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1cb6c7-f26c-4f6f-829b-c716e523c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28acbced-6843-4ab6-9bb3-5838a50cdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in audio text extraction: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Predicted emotion from image: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Predicted emotion from audio: Neutral\n",
      "Audio prediction is 'Neutral'. Shifting priority to image prediction.\n",
      "Final predicted emotion (after majority check): Anger\n",
      "Final predicted emotion from video: Anger\n"
     ]
    }
   ],
   "source": [
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "        audio_emotion = None\n",
    "        image_emotion = None\n",
    "        text_emotion = None\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # Handle Neutral audio or text emotion: if \"Neutral\", shift priority to Neutral\n",
    "        if audio_emotion == \"Neutral\" and text_emotion == \"Neutral\":\n",
    "            print(\"Audio and Text are both 'Neutral'. Prioritizing Neutral prediction.\")\n",
    "            predictions = [\"Neutral\"]  # Prioritize Neutral regardless of image prediction\n",
    "\n",
    "        # Handle Neutral audio emotion: if \"Neutral\", shift priority to image\n",
    "        elif audio_emotion == \"Neutral\" and image_emotion is not None:\n",
    "            print(f\"Audio prediction is 'Neutral'. Shifting priority to image prediction.\")\n",
    "            predictions = [image_emotion]  # Ignore the Neutral audio emotion and use the image emotion\n",
    "        \n",
    "        # Majority voting logic: check all predictions\n",
    "        if predictions:\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion (after majority check): {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        image_path = None\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            # Save only the first frame as an image for simplicity\n",
    "            if frame_count == 1:\n",
    "                image_path = \"temp_image.jpg\"\n",
    "                cv2.imwrite(image_path, frame)\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Predict sentiment using the extracted audio, text, and image\n",
    "        text_input = extract_text_from_audio(audio_path)  # Extract text from the audio\n",
    "        final_emotion = self.predict(text=text_input, image_path=image_path, audio_path=audio_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "        return final_emotion\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with a video input\n",
    "video_input_path = \"./video_data/Anger/Angry Man Free Stock Footage _ Man yells at woman Angry _ No Copyright _.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check the final prediction\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1215fbb-f27a-4648-8095-63f3435128c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in audio text extraction: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Predicted emotion from image: Sadness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "Final predicted emotion (after majority check): Anger\n",
      "Final predicted emotion from video: Anger\n"
     ]
    }
   ],
   "source": [
    "# Test with a video input\n",
    "video_input_path = \"./video_data/Anger/Just an Act👿👿😤#newshorts#art#english#dialogue#angry.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f664f8fb-069c-401b-b064-2330463220f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 770ms/step\n",
      "Predicted emotion from text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Predicted emotion from image: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted emotion from audio: Neutral\n",
      "Audio and Text are both 'Neutral'. Prioritizing Neutral prediction.\n",
      "Final predicted emotion (after majority check): Neutral\n",
      "Final predicted emotion from video: Neutral\n"
     ]
    }
   ],
   "source": [
    "# Test with a video input\n",
    "video_input_path = \"./video_data/Disgust/disgust.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a3ad777-732a-47a5-9e49-2511fb83fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 755ms/step\n",
      "Predicted emotion from text: Sadness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Predicted emotion from image: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Predicted emotion from audio: Sadness\n",
      "Final predicted emotion (after majority check): Sadness\n",
      "Final predicted emotion from video: Sadness\n"
     ]
    }
   ],
   "source": [
    "   # Test with a video input\n",
    "video_input_path = \"./video_data/Sadness/I am problem - sad  English status.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df9fcb4a-7249-4d4f-93ac-9cd123921f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
      "Predicted emotion from text: Fear\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Predicted emotion from image: Happiness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "Final predicted emotion (after majority check): Happiness\n",
      "Final predicted emotion from video: Happiness\n"
     ]
    }
   ],
   "source": [
    "   # Test with a video input\n",
    "video_input_path = \"./video_data/Happiness/videoplayback (2).mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32926cfe-bf36-4e17-875d-92a22d589fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6c91b-4de8-40de-a132-7256455ee2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce35c5b-2d38-40f8-a280-32669fb71853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd8b4173-9e89-4131-a760-13a5c75a256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Error in text prediction: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:832 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[0] = 0 is not in [0, 0)\n",
      "\t [[{{node RaggedGather/RaggedGather}}]] [Op:IteratorGetNext] name: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Predicted emotion from image: Happiness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
      "Predicted emotion from audio: Sadness\n",
      "Final predicted emotion (after majority check): Happiness\n",
      "Final predicted emotion from video: Happiness\n"
     ]
    }
   ],
   "source": [
    "   # Test with a video input\n",
    "video_input_path = \"./video_data/Happiness/The Easiest Monologue in the World! All Emotions in 1 Minute!.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "235a074c-8f47-4d0f-aedc-7f93b0e03c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Error in audio text extraction: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Predicted emotion from image: Surprise\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "Predicted emotion from audio: Neutral\n",
      "Audio prediction is 'Neutral'. Shifting priority to image prediction.\n",
      "Final predicted emotion (after majority check): Surprise\n",
      "Final predicted emotion from video: Surprise\n"
     ]
    }
   ],
   "source": [
    "   # Test with a video input\n",
    "video_input_path = \"./video_data/Happiness/videoplayback.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e15a3b-3a53-4c85-9355-63aadb3f486d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bb2c8-4338-4bbc-bc48-2ca382bf7284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506eed9-ff49-4b7b-8c7f-b3d1595afa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0b9c7-122c-4c24-be3f-250f2363e19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4032e8-bcbb-4c72-b1de-f4785d565934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e11e9-d8ba-4f95-a71a-f160f363cf89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca682b5-7ff8-42e8-b7c0-0e02deb01551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e0489-1ba0-4c3e-a228-e6b9b8ebda17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4b4f7-ef68-4ae7-b427-cd11c37de63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87969f-1160-412c-ac36-4d80fe3832c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae161f69-0dcc-4e9d-abaf-233cca47d1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af1970-4084-4855-82ce-5dd1e95defe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c38e9-b1c8-42d0-9c32-32ddc3fb2a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61810f8b-a544-4297-af52-ba83b57352e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3df95-7eda-47d2-a171-6e3353c76cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05283fdf-6014-4ec3-ae6b-bca70203dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ff850-19b0-427f-a73e-69a8db548ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878600a-f7d4-4ff8-9ab5-7e8e606709e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2c817-b50a-42fa-b3de-b99e057960d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7853c5-f79f-40fb-b548-e80ca4774d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb300c3e-6ac0-4f46-b5a7-25ee1c7f48d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3938c32-b42a-43fb-9cef-706657a2778f",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31927f32-9322-4ecb-8cb1-01db4e0fda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "import os\n",
    "\n",
    "# Global flag for stopping audio recording\n",
    "recording_active = True\n",
    "\n",
    "# Function to record audio in the background\n",
    "def record_audio():\n",
    "    global recording_active\n",
    "    fs = 16000  # Sample rate\n",
    "    duration = 5  # Record for 5 seconds at a time\n",
    "    while recording_active:  # Loop while recording_active is True\n",
    "        print(\"Recording audio...\")\n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "        sd.wait()\n",
    "\n",
    "        # Save recorded audio to a WAV file\n",
    "        with wave.open(\"temp_audio.wav\", 'wb') as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(2)\n",
    "            wf.setframerate(fs)\n",
    "            wf.writeframes(recording.tobytes())\n",
    "        print(\"Audio recorded and saved.\")\n",
    "\n",
    "# Function to stop all recording processes\n",
    "def stop_all_recording():\n",
    "    global recording_active\n",
    "    recording_active = False  # This will stop the audio recording thread\n",
    "    print(\"Audio recording stopped.\")\n",
    "\n",
    "# Start the audio recording in a separate thread\n",
    "audio_thread = threading.Thread(target=record_audio, daemon=True)\n",
    "audio_thread.start()\n",
    "\n",
    "# Wait for the user to stop the recording\n",
    "input(\"Press Enter to stop all recording...\")\n",
    "\n",
    "# Call the function to stop recording\n",
    "stop_all_recording()\n",
    "\n",
    "# Wait for the audio recording thread to finish\n",
    "audio_thread.join()\n",
    "\n",
    "print(\"All processes stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfd01a-da2f-44e4-9845-0157600053d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "import os\n",
    "\n",
    "# Global flag for stopping audio recording\n",
    "recording_active = True\n",
    "\n",
    "# Function to record audio in the background\n",
    "def record_audio():\n",
    "    global recording_active\n",
    "    fs = 16000  # Sample rate\n",
    "    duration = 5  # Record for 5 seconds at a time\n",
    "    while recording_active:  # Loop while recording_active is True\n",
    "        print(\"Recording audio...\")\n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "        sd.wait()\n",
    "\n",
    "        # Save recorded audio to a WAV file\n",
    "        with wave.open(\"temp_audio.wav\", 'wb') as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(2)\n",
    "            wf.setframerate(fs)\n",
    "            wf.writeframes(recording.tobytes())\n",
    "        print(\"Audio recorded and saved.\")\n",
    "\n",
    "# Function to stop all recording processes\n",
    "def stop_all_recording():\n",
    "    global recording_active\n",
    "    recording_active = False  # This will stop the audio recording thread\n",
    "    print(\"Audio recording stopped.\")\n",
    "\n",
    "# Start the audio recording in a separate thread\n",
    "audio_thread = threading.Thread(target=record_audio, daemon=True)\n",
    "audio_thread.start()\n",
    "\n",
    "# Wait for the user to stop the recording\n",
    "input(\"Press Enter to stop all recording...\")\n",
    "\n",
    "# Call the function to stop recording\n",
    "stop_all_recording()\n",
    "\n",
    "# Wait for the audio recording thread to finish\n",
    "audio_thread.join()\n",
    "\n",
    "print(\"All processes stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fb1c9-fbce-4f8f-bd81-caa642d194a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0ef9c-1b1c-47dc-97fd-391ee5eab434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbbd6339-ea46-4e3e-a75e-1bef4944cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Audio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\speech_recognition\\__init__.py:238\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# attempt to read the file as WAV\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m wave\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename_or_fileobject, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# RIFF WAV is a little-endian format (most ``audioop`` operations assume that the frames are stored in little-endian form)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\wave.py:649\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Wave_read(f)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\wave.py:286\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitfp(f)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\wave.py:251\u001b[0m, in \u001b[0;36mWave_read.initfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soundpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m _Chunk(file, bigendian \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mgetname() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\wave.py:117\u001b[0m, in \u001b[0;36m_Chunk.__init__\u001b[1;34m(self, file, align, bigendian, inclheader)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunkname) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mEOFError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\speech_recognition\\__init__.py:243\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m# attempt to read the file as AIFF\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename_or_fileobject, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# AIFF is a big-endian format\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\aifc.py:954\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Aifc_read(f)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\aifc.py:358\u001b[0m, in \u001b[0;36mAifc_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitfp(file_object)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\aifc.py:320\u001b[0m, in \u001b[0;36mAifc_read.initfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m file\n\u001b[1;32m--> 320\u001b[0m chunk \u001b[38;5;241m=\u001b[39m Chunk(file)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mgetname() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFORM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\chunk.py:67\u001b[0m, in \u001b[0;36mChunk.__init__\u001b[1;34m(self, file, align, bigendian, inclheader)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunkname) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mEOFError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\speech_recognition\\__init__.py:269\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(aiff_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (aifc\u001b[38;5;241m.\u001b[39mError, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\aifc.py:954\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Aifc_read(f)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\aifc.py:364\u001b[0m, in \u001b[0;36mAifc_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;66;03m# assume it is an open file object already\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitfp(f)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\aifc.py:320\u001b[0m, in \u001b[0;36mAifc_read.initfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m file\n\u001b[1;32m--> 320\u001b[0m chunk \u001b[38;5;241m=\u001b[39m Chunk(file)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mgetname() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFORM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\chunk.py:67\u001b[0m, in \u001b[0;36mChunk.__init__\u001b[1;34m(self, file, align, bigendian, inclheader)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunkname) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mEOFError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 207\u001b[0m\n\u001b[0;32m    200\u001b[0m multi_modal_model \u001b[38;5;241m=\u001b[39m MultiModalSentimentAnalysis(\n\u001b[0;32m    201\u001b[0m     audio_model\u001b[38;5;241m=\u001b[39mAudioPredictor(),\n\u001b[0;32m    202\u001b[0m     text_model\u001b[38;5;241m=\u001b[39mtext_classifier,\n\u001b[0;32m    203\u001b[0m     image_model\u001b[38;5;241m=\u001b[39mimage_predictor\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Start live stream for real-time sentiment prediction\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m multi_modal_model\u001b[38;5;241m.\u001b[39mlive_stream()\n",
      "Cell \u001b[1;32mIn[84], line 176\u001b[0m, in \u001b[0;36mMultiModalSentimentAnalysis.live_stream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m image_emotion \u001b[38;5;241m=\u001b[39m emotion_labels_img[\u001b[38;5;28mint\u001b[39m(image_prediction)]\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Process audio data\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m text_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_audio_data(audio_queue)  \u001b[38;5;66;03m# Convert audio to text\u001b[39;00m\n\u001b[0;32m    177\u001b[0m final_emotion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(text\u001b[38;5;241m=\u001b[39mtext_input, image\u001b[38;5;241m=\u001b[39mframe, audio_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Display the predicted emotion\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[84], line 145\u001b[0m, in \u001b[0;36mMultiModalSentimentAnalysis.process_audio_data\u001b[1;34m(self, audio_queue)\u001b[0m\n\u001b[0;32m    142\u001b[0m     audio_path \u001b[38;5;241m=\u001b[39m tmp_file\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Extract text from the audio\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m text_input \u001b[38;5;241m=\u001b[39m extract_text_from_audio(audio_path)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Clean up temporary audio file\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audio_path):\n",
      "Cell \u001b[1;32mIn[84], line 33\u001b[0m, in \u001b[0;36mextract_text_from_audio\u001b[1;34m(audio_file)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_audio\u001b[39m(audio_file):\n\u001b[0;32m     32\u001b[0m     recognizer \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mAudioFile(audio_file) \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m     34\u001b[0m         audio_data \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecord(source)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\speech_recognition\\__init__.py:271\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(aiff_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (aifc\u001b[38;5;241m.\u001b[39mError, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n\u001b[1;32m--> 271\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# AIFF is a big-endian format\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader\u001b[38;5;241m.\u001b[39mgetnchannels() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio must be mono or stereo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Audio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import speech_recognition as sr\n",
    "from collections import Counter\n",
    "from keras.preprocessing import image\n",
    "import librosa\n",
    "from moviepy.editor import VideoFileClip\n",
    "import tempfile\n",
    "import os\n",
    "from queue import Queue\n",
    "from tensorflow.keras.models import load_model  # Ensure Keras models are imported correctly\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)  # Ensure this loads your image model\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "class AudioPredictor:\n",
    "    def audio_classify(self, audio_path):\n",
    "        # Implement actual audio classification logic here using your model\n",
    "        # Example using librosa to extract features\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        features = librosa.feature.mfcc(audio, sr=sr)\n",
    "        features = np.mean(features, axis=1).reshape(1, -1)  # Feature extraction\n",
    "        prediction = audio_classifier.predict(features)\n",
    "        return prediction[0]  # Return the predicted emotion class\n",
    "\n",
    "\n",
    "class ImagePredictor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess_image(self, image_array):\n",
    "        # Resize the image to match the input size of your model (e.g., 299x299 for InceptionV3)\n",
    "        img = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB if using OpenCV\n",
    "        img = cv2.resize(img, (299, 299))  # Resize to the desired shape\n",
    "        img_array = image.img_to_array(img)  # Convert image to array\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        img_array /= 255.0  # Normalize the image\n",
    "        return img_array\n",
    "\n",
    "    def image_classify(self, image_array):\n",
    "        # Preprocess the image\n",
    "        processed_image = self.preprocess_image(image_array)\n",
    "        \n",
    "        # Predict the emotion\n",
    "        predictions = self.model.predict(processed_image)  # Ensure this works for your model\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return predicted_class[0]  # Return the predicted class index\n",
    "\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # If all predictions are different, prioritize the image prediction\n",
    "        if len(predictions) == 3 and len(set(predictions)) == 3:  # All predictions are different\n",
    "            print(f\"Different emotions predicted from all models. Prioritizing image prediction.\")\n",
    "            return predictions[1]  # Image prediction is in the second position (index 1)\n",
    "        \n",
    "        # Majority voting logic if predictions are the same or not all different\n",
    "        if len(predictions) > 0:\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion: {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_audio_data(self, audio_queue):\n",
    "        # Initialize an empty byte string to hold the audio data\n",
    "        audio_data = b''\n",
    "\n",
    "        # Continue to process the audio queue until there's data\n",
    "        while not audio_queue.empty():\n",
    "            audio_data += audio_queue.get()\n",
    "\n",
    "        # Save the audio data to a temporary .wav file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "            tmp_file.write(audio_data)\n",
    "            audio_path = tmp_file.name\n",
    "    \n",
    "        # Extract text from the audio\n",
    "        text_input = extract_text_from_audio(audio_path)\n",
    "    \n",
    "        # Clean up temporary audio file\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "\n",
    "        return text_input\n",
    "\n",
    "    def live_stream(self):\n",
    "        # Create video capture and audio stream\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not access the webcam.\")\n",
    "            return\n",
    "\n",
    "        # Create a queue for audio data\n",
    "        audio_queue = Queue()\n",
    "        audio_thread = threading.Thread(target=self.capture_audio, args=(audio_queue,))\n",
    "        audio_thread.daemon = True\n",
    "        audio_thread.start()\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process image (live face frame)\n",
    "            image_prediction = self.image_model.image_classify(frame)\n",
    "            image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "\n",
    "            # Process audio data\n",
    "            text_input = self.process_audio_data(audio_queue)  # Convert audio to text\n",
    "            final_emotion = self.predict(text=text_input, image=frame, audio_data=None)\n",
    "\n",
    "            # Display the predicted emotion\n",
    "            cv2.putText(frame, f\"Predicted Emotion: {final_emotion}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Live Sentiment Analysis\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def capture_audio(self, audio_queue):\n",
    "        recognizer = sr.Recognizer()\n",
    "        microphone = sr.Microphone()\n",
    "\n",
    "        while True:\n",
    "            with microphone as source:\n",
    "                audio_data = recognizer.listen(source)\n",
    "                audio_queue.put(audio_data.get_wav_data())  # Store audio data for processing\n",
    "\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=AudioPredictor(),\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Start live stream for real-time sentiment prediction\n",
    "multi_modal_model.live_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584178de-2fae-4fd1-bd73-f1871d798172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6f931-bb63-4e75-9c71-c0619e80854d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7dba592-821c-4184-a030-d464a69db7cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DummyAudioModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 140\u001b[0m\n\u001b[0;32m    136\u001b[0m                 audio_queue\u001b[38;5;241m.\u001b[39mput(audio_data)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Instantiate the multi-modal sentiment analysis model with dummy models\u001b[39;00m\n\u001b[0;32m    139\u001b[0m multi_modal_model \u001b[38;5;241m=\u001b[39m MultiModalSentimentAnalysis(\n\u001b[1;32m--> 140\u001b[0m     audio_model\u001b[38;5;241m=\u001b[39mDummyAudioModel(),\n\u001b[0;32m    141\u001b[0m     text_model\u001b[38;5;241m=\u001b[39mDummyTextModel(),\n\u001b[0;32m    142\u001b[0m     image_model\u001b[38;5;241m=\u001b[39mDummyImageModel()\n\u001b[0;32m    143\u001b[0m )\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Start live stream for real-time sentiment prediction\u001b[39;00m\n\u001b[0;32m    146\u001b[0m multi_modal_model\u001b[38;5;241m.\u001b[39mlive_stream()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DummyAudioModel' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import tempfile\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Assuming emotion labels are predefined\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "emotion_labels_audio = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image=None, audio_data=None):\n",
    "        predictions = []\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_data is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_data)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # Majority voting logic if predictions are the same or not all different\n",
    "        if len(predictions) > 0:\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion: {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_audio_data(self, audio_queue):\n",
    "        # Process captured audio data for sentiment prediction\n",
    "        audio_data = b''\n",
    "        while not audio_queue.empty():\n",
    "            audio_data += audio_queue.get()  # Get the audio data from the queue\n",
    "\n",
    "        if not audio_data:\n",
    "            return \"\"  # Return empty if no audio data is available\n",
    "\n",
    "        # Save the audio data to a temporary .wav file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "            tmp_file.write(audio_data)\n",
    "            tmp_filename = tmp_file.name\n",
    "\n",
    "        # Use speech recognition to convert audio to text\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(tmp_filename) as source:\n",
    "            audio = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            os.remove(tmp_filename)  # Clean up the temporary file\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error in speech recognition: {e}\")\n",
    "            os.remove(tmp_filename)  # Clean up the temporary file\n",
    "            return \"\"\n",
    "\n",
    "    def live_stream(self):\n",
    "        # Create video capture and audio stream\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not access the webcam.\")\n",
    "            return\n",
    "        \n",
    "        # Create a queue for audio data\n",
    "        audio_queue = Queue()\n",
    "        audio_thread = threading.Thread(target=self.capture_audio, args=(audio_queue,))\n",
    "        audio_thread.daemon = True\n",
    "        audio_thread.start()\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process image (live face frame)\n",
    "            image_prediction = self.image_model.image_classify(frame)\n",
    "            image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "\n",
    "            # Process audio data\n",
    "            text_input = self.process_audio_data(audio_queue)  # Convert audio to text\n",
    "            final_emotion = self.predict(text=text_input, image=frame, audio_data=None)\n",
    "\n",
    "            # Display the predicted emotion\n",
    "            cv2.putText(frame, f\"Predicted Emotion: {final_emotion}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Live Sentiment Analysis\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def capture_audio(self, audio_queue):\n",
    "        # Use SpeechRecognition to capture live audio from the microphone\n",
    "        recognizer = sr.Recognizer()\n",
    "        mic = sr.Microphone()\n",
    "\n",
    "        with mic as source:\n",
    "            while True:\n",
    "                audio = recognizer.listen(source)\n",
    "                audio_data = audio.get_wav_data()\n",
    "                audio_queue.put(audio_data)\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model with dummy models\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=DummyAudioModel(),\n",
    "    text_model=DummyTextModel(),\n",
    "    image_model=DummyImageModel()\n",
    ")\n",
    "\n",
    "# Start live stream for real-time sentiment prediction\n",
    "multi_modal_model.live_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a30d9-54bb-4203-8b95-b6393434d523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0ac8d3e-cc5c-46b9-93cb-abd73033774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "else:\n",
    "    print(\"Press 'q' to quit.\")\n",
    "\n",
    "# Frame count for naming captured images\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw rectangles around the detected faces and capture the face image\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        face = frame[y:y + h, x:x + w]\n",
    "        frame_count += 1\n",
    "        \n",
    "        # Save the captured face image\n",
    "        cv2.imwrite(f'face_{frame_count}.jpg', face)\n",
    "\n",
    "    # Display the frame with rectangles drawn\n",
    "    cv2.imshow('Face Capture', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c709e1-e705-4f2f-a451-f8e6645b158e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa499faf-3c2f-4627-8cbc-6100cf879f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bcc94-e594-4544-a86c-eb2669af2b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dafbe3-4786-4696-aae9-b30a1a176800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89949a5a-b5f6-4ab5-bf83-8018b08d27a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145d8b3b-71cd-46d5-b9ec-7004434c123c",
   "metadata": {},
   "source": [
    "# ____________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76305f2-65a3-4534-8a14-1a93c9060ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9e592-6fe5-4c24-8e6c-8d7da1419452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c71e7cf-9dd6-432d-a48e-579405e250fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in audio text extraction: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Predicted emotion from image: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Predicted emotion from audio: Neutral\n",
      "Final predicted emotion: Neutral\n",
      "Final predicted emotion from video: Neutral\n"
     ]
    }
   ],
   "source": [
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # If all predictions are different, prioritize the image prediction\n",
    "        if len(predictions) == 3 and len(set(predictions)) == 3:  # All predictions are different\n",
    "            print(f\"Different emotions predicted from all models. Prioritizing image prediction.\")\n",
    "            return predictions[0]  # Image prediction is in the second position (index 1)\n",
    "        \n",
    "        # Majority voting logic if predictions are the same or not all different\n",
    "        if len(predictions) > 0:\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion: {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        image_path = None\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            # Save only the first frame as an image for simplicity\n",
    "            if frame_count == 1:\n",
    "                image_path = \"temp_image.jpg\"\n",
    "                cv2.imwrite(image_path, frame)\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Predict sentiment using the extracted audio, text, and image\n",
    "        text_input = extract_text_from_audio(audio_path)  # Extract text from the audio\n",
    "        final_emotion = self.predict(text=text_input, image_path=image_path, audio_path=audio_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "        return final_emotion\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with a video input\n",
    "video_input_path = \"./video_data/Anger/Angry Man Free Stock Footage _ Man yells at woman Angry _ No Copyright _.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "\n",
    "# Check if the final prediction contains two different emotions\n",
    "if isinstance(final_prediction, list):\n",
    "    print(f\"Final predicted emotions from image, audio, and text: {final_prediction}\")\n",
    "else:\n",
    "    print(f\"Final predicted emotion from video: {final_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43939b0-8dad-42c9-8ca1-fce1ce738c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Disgust\n",
      "Final predicted emotion: Neutral\n",
      "Final predicted emotion from video: Neutral\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import VideoFileClip\n",
    "from collections import Counter\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def priority_majority_fusion(self, predictions, text_prediction=None):\n",
    "        # Priority-majority fusion logic\n",
    "        if len(set(predictions)) == 3:\n",
    "            if text_prediction == \"Neutral\":\n",
    "                return \"Neutral\"\n",
    "            return predictions[2]\n",
    "        \n",
    "        most_common_emotion, _ = Counter(predictions).most_common(1)[0]\n",
    "        \n",
    "        if text_prediction == \"Neutral\" and most_common_emotion != \"Neutral\":\n",
    "            return most_common_emotion\n",
    "\n",
    "        return most_common_emotion\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "        text_prediction = None\n",
    "\n",
    "        # Extract and predict from text if audio is provided\n",
    "        if audio_path is not None:\n",
    "            extracted_text = extract_text_from_audio(audio_path)\n",
    "            \n",
    "            if extracted_text:\n",
    "                text_prediction = self.text_model.text_classify(extracted_text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from extracted text: {text_emotion}\")\n",
    "            else:\n",
    "                print(\"No text extracted from audio. Proceeding with audio and image prediction.\")\n",
    "                try:\n",
    "                    audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                    audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                    predictions.append(audio_emotion)\n",
    "                    print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in audio prediction: {e}\")\n",
    "\n",
    "        # Predict from image if image path is provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Apply fusion logic if predictions are available\n",
    "        if len(predictions) > 0:\n",
    "            final_emotion = self.priority_majority_fusion(predictions, text_prediction)\n",
    "            print(f\"Final predicted emotion: {final_emotion}\")\n",
    "            return final_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        image_path = None\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            # Save only the first frame as an image for simplicity\n",
    "            if frame_count == 1:\n",
    "                image_path = \"temp_image.jpg\"\n",
    "                cv2.imwrite(image_path, frame)\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Predict sentiment using the extracted audio, text, and image\n",
    "        final_emotion = self.predict(image_path=image_path, audio_path=audio_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "        return final_emotion\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "video_input_path = \"./Angry.mp4\"\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "print(f\"Final predicted emotion from video: {final_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d106a-c21a-43f8-9a51-754ac4aa6e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bc105-9b17-4c71-81b4-442be8898223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e774dddf-8fcb-4ef1-9810-97d0c51aedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856ms/step\n",
      "Predicted emotion from text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "Predicted emotion from audio: Sadness\n",
      "Final predicted emotion: Disgust\n",
      "Final predicted emotion from video: Disgust\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # Combine predictions (if more than one prediction is available)\n",
    "        if len(predictions) > 0:\n",
    "            # Majority voting logic\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion: {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        image_path = None\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            # Save only the first frame as an image for simplicity\n",
    "            if frame_count == 1:\n",
    "                image_path = \"temp_image.jpg\"\n",
    "                cv2.imwrite(image_path, frame)\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Predict sentiment using the extracted audio, text, and image\n",
    "        text_input = extract_text_from_audio(audio_path)  # Extract text from the audio\n",
    "        final_emotion = self.predict(text=text_input, image_path=image_path, audio_path=audio_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "        return final_emotion\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with a video input\n",
    "video_input_path = \"./Sad.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "print(f\"Final predicted emotion from video: {final_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101da3b8-9ad5-4371-9118-2549a15575fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "492839a9-ca9d-4d4d-8c62-53b2a554fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in audio text extraction: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Surprise\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
      "Predicted emotion from audio: Sadness\n",
      "Final predicted emotion: Sadness\n",
      "Final predicted emotion from video: Sadness\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # Combine predictions (if more than one prediction is available)\n",
    "        if len(predictions) > 0:\n",
    "            # Majority voting logic\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion: {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        image_path = None\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "\n",
    "            # Display video frames in real-time\n",
    "            cv2.imshow('Processing Video', frame)\n",
    "\n",
    "            # Save only the first frame as an image for simplicity\n",
    "            if frame_count == 1:\n",
    "                image_path = \"temp_image.jpg\"\n",
    "                cv2.imwrite(image_path, frame)\n",
    "            \n",
    "            # Check for 'q' key to stop the video window if needed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Predict sentiment using the extracted audio, text, and image\n",
    "        text_input = extract_text_from_audio(audio_path)  # Extract text from the audio\n",
    "        final_emotion = self.predict(text=text_input, image_path=image_path, audio_path=audio_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "        return final_emotion\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with a video input\n",
    "video_input_path = \"./Fear.mp4\"  # Replace with your video file path\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "print(f\"Final predicted emotion from video: {final_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335f352-429a-46b5-b1b0-30c12dd6e677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ba0bf-2074-4703-98b9-4ef53fb42ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec8637-96ad-4e2f-ac11-a363a5e5a11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d496d-e5c9-4853-b5d7-bcdaf751f3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "150dea7e-377e-4868-a893-2dc190d9d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 963ms/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "Predicted emotion from audio features: Sadness\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Disgust\n",
      "Final predicted emotion: Neutral\n",
      "Final predicted emotion from video: Neutral\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import VideoFileClip\n",
    "from collections import Counter\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def majority_fusion(self, predictions):\n",
    "        # Majority voting for predictions\n",
    "        most_common_emotion, _ = Counter(predictions).most_common(1)[0]\n",
    "        return most_common_emotion\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "        text_prediction = None\n",
    "\n",
    "        # Extract and predict from audio if audio is provided\n",
    "        if audio_path is not None:\n",
    "            extracted_text = extract_text_from_audio(audio_path)\n",
    "            \n",
    "            if extracted_text:\n",
    "                # Predict sentiment using text classifier from extracted text\n",
    "                text_prediction = self.text_model.text_classify(extracted_text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from extracted text: {text_emotion}\")\n",
    "            else:\n",
    "                print(\"No text extracted from audio. Proceeding with audio and image prediction.\")\n",
    "            \n",
    "            try:\n",
    "                # Predict sentiment using audio classifier from raw audio features\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio features: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "\n",
    "        # Predict from image if image path is provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                # Predict sentiment using image classifier\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "\n",
    "        # Apply fusion logic if predictions are available\n",
    "        if len(predictions) > 0:\n",
    "            final_emotion = self.majority_fusion(predictions)\n",
    "            print(f\"Final predicted emotion: {final_emotion}\")\n",
    "            return final_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        \n",
    "        # Extract frames from video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        image_path = None\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            # Save only the first frame as an image for simplicity\n",
    "            if frame_count == 1:\n",
    "                image_path = \"temp_image.jpg\"\n",
    "                cv2.imwrite(image_path, frame)\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # Predict sentiment using the extracted audio, text, and image\n",
    "        final_emotion = self.predict(image_path=image_path, audio_path=audio_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "        return final_emotion\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "video_input_path = \"./Sad.mp4\"\n",
    "final_prediction = multi_modal_model.process_video(video_input_path)\n",
    "print(f\"Final predicted emotion from video: {final_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0549ef0-3d8c-431e-9099-16a246f627e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879db8fa-f210-430b-97d4-47a32ebdbcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2591f0bc-6cb3-4b96-88fb-054e6c136c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858ms/step\n",
      "Predicted emotion from text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "Predicted emotion from audio: Neutral\n",
      "Error in image prediction for a frame: path should be path-like or io.BytesIO, not <class 'numpy.ndarray'>\n",
      "Error in image prediction for a frame: path should be path-like or io.BytesIO, not <class 'numpy.ndarray'>\n",
      "Error in image prediction for a frame: path should be path-like or io.BytesIO, not <class 'numpy.ndarray'>\n",
      "Error in image prediction for a frame: path should be path-like or io.BytesIO, not <class 'numpy.ndarray'>\n",
      "Final predicted emotion: Neutral\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import VideoFileClip\n",
    "from collections import Counter\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Load pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract audio from video and return as variable\n",
    "def extract_audio_from_video(video_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    audio.write_audiofile(audio_path, codec='pcm_s16le')\n",
    "    return audio_path\n",
    "\n",
    "# Function to extract frames from video and return as a list of variables\n",
    "def extract_frames_from_video(video_path, frame_interval=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def priority_majority_fusion(self, predictions, text_prediction=None):\n",
    "        if len(set(predictions)) == len(predictions):\n",
    "            if text_prediction == \"Neutral\":\n",
    "                return \"Neutral\"\n",
    "            return predictions[-1]\n",
    "\n",
    "        most_common_emotion, _ = Counter(predictions).most_common(1)[0]\n",
    "        if text_prediction == \"Neutral\" and most_common_emotion != \"Neutral\":\n",
    "            return most_common_emotion\n",
    "        return most_common_emotion\n",
    "\n",
    "    def predict(self, audio_path=None, frames=None, text=None):\n",
    "        predictions = []\n",
    "        text_prediction = None\n",
    "\n",
    "        # Text prediction\n",
    "        if text:\n",
    "            text_prediction = self.text_model.text_classify(text)\n",
    "            text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "            predictions.append(text_emotion)\n",
    "            print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "\n",
    "        # Audio prediction\n",
    "        if audio_path:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "\n",
    "        # Image prediction\n",
    "        if frames:\n",
    "            frame_emotions = []\n",
    "            for frame in frames:\n",
    "                try:\n",
    "                    frame_prediction = self.image_model.image_classify(frame)\n",
    "                    frame_emotion = emotion_labels_img[int(frame_prediction)]\n",
    "                    frame_emotions.append(frame_emotion)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in image prediction for a frame: {e}\")\n",
    "            if frame_emotions:\n",
    "                most_common_frame_emotion, _ = Counter(frame_emotions).most_common(1)[0]\n",
    "                predictions.append(most_common_frame_emotion)\n",
    "                print(f\"Most common emotion from frames: {most_common_frame_emotion}\")\n",
    "\n",
    "        # Determine final prediction\n",
    "        if predictions:\n",
    "            final_emotion = self.priority_majority_fusion(predictions, text_prediction)\n",
    "            print(f\"Final predicted emotion: {final_emotion}\")\n",
    "            return final_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "audio_path = extract_audio_from_video(\"./Angry.mp4\")\n",
    "frames = extract_frames_from_video(\"./Angry.mp4\")\n",
    "text_from_audio = extract_text_from_audio(audio_path)\n",
    "\n",
    "# Initialize the multi-modal model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Get the final prediction\n",
    "final_prediction = multi_modal_model.predict(audio_path=audio_path, frames=frames, text=text_from_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfb131-f455-457f-b129-b5dda55f164b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
