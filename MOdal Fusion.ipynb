{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9920583c-ce66-4bb0-9eb7-1a91debb4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from predictor_all import AudioPredictor\n",
    "from predictor_all import VoteClassifier\n",
    "from predictor_all import ImagePredictor\n",
    "from keras.models import load_model\n",
    "import speech_recognition as sr\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import image\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b46a31c-40ed-4adc-b6a0-ccc25b219c06",
   "metadata": {},
   "source": [
    "## Information for prediction\n",
    "\n",
    "### Angry :\n",
    "####       img: ./Image/6 Emotions for image classification/anger.jpg/gv232bwvvkb8m27mh1.jpg\n",
    "####       audio: ./AUDIO/TESS Toronto emotional speech set data/OAF_angry/OAF_burn_angry.wav\n",
    "####       text : I will burn u\n",
    "\n",
    "### Happy :\n",
    "####       img: ./Image/6 Emotions for image classification/happy.jpg/mature-woman-standing-outside-in-a-park-and-smiling.jpg\n",
    "####       audio: ./AUDIO/TESS Toronto emotional speech set data/OAF_happy/OAF_love_happy.wav\n",
    "####       text: I am feeling really happy today!\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "758328a1-79a6-4ab4-b9f3-f438b854f3a7",
   "metadata": {},
   "source": [
    "audio_classifier_model.pkl\n",
    "text_classifier_model.pkl\n",
    "image_predictor.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e6e83c-a8ff-49f6-8358-8ecfd728ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./Image/6 Emotions for image classification/anger.jpg/gv232bwvvkb8m27mh1.jpg\"\n",
    "text_input = \"i am very angry of u i will burn u\"\n",
    "audio_path = \"./AUDIO/TESS Toronto emotional speech set data/OAF_angry/OAF_burn_angry.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f7ca3db-c5c8-4d36-a5db-83bffca49b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "Final predicted emotion: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "Predicted emotion from text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "Final predicted emotion: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "class MultiModalSentimentAnalysiss:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "\n",
    "        # Predict from text if provided\n",
    "        if text is not None:\n",
    "            try:\n",
    "                text_prediction = self.text_model.text_classify(text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from text: {text_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text prediction: {e}\")\n",
    "        \n",
    "        # Predict from image if provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # Predict from audio if provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "        \n",
    "        # Combine predictions (if more than one prediction is available)\n",
    "        if len(predictions) > 0:\n",
    "            # Majority voting logic or other combining logic can be applied here\n",
    "            most_common_emotion = max(set(predictions), key=predictions.count)\n",
    "            print(f\"Final predicted emotion: {most_common_emotion}\")\n",
    "            return most_common_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysiss(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with all data present (text, image, and audio)\n",
    "\n",
    "image_input_path = \"./Image/6 Emotions for image classification/anger.jpg/gv232bwvvkb8m27mh1.jpg\"\n",
    "audio_input_path = \"./AUDIO/TESS Toronto emotional speech set data/OAF_angry/OAF_burn_angry.wav\"\n",
    "text_input = extract_text_from_audio(audio_input_path)\n",
    "\n",
    "final_prediction = multi_modal_model.predict(text=text_input, image_path=image_input_path, audio_path=audio_input_path)\n",
    "\n",
    "# Test with missing data (for example, only text and audio)\n",
    "final_prediction = multi_modal_model.predict(text=text_input, audio_path=audio_input_path)\n",
    "\n",
    "# Test with only one modality (image)\n",
    "final_prediction = multi_modal_model.predict(image_path=image_input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0304b98-3e68-4b2f-a2b4-63c6b0b576d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80211913-138b-4eb7-a812-6134a9b39250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db6748c-7cbe-4b85-a15c-a71ee68e69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6df65a4b-ef53-4982-afe5-bc588c053799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def priority_majority_fusion(self, predictions, text_prediction=None):\n",
    "        # Priority-majority fusion logic:\n",
    "\n",
    "        # If all predictions are different, prioritize image only if all are unequal and text is \"Neutral\"\n",
    "        if len(set(predictions)) == 3:\n",
    "            if text_prediction == \"Neutral\":\n",
    "                return \"Neutral\"  # If text is Neutral, return Neutral\n",
    "            return predictions[2]  # Otherwise, return the image prediction if all are different\n",
    "        \n",
    "        # Majority voting logic\n",
    "        most_common_emotion, _ = Counter(predictions).most_common(1)[0]\n",
    "        \n",
    "        # If text is \"Neutral\" and the majority is different, return majority, not neutral\n",
    "        if text_prediction == \"Neutral\" and most_common_emotion != \"Neutral\":\n",
    "            return most_common_emotion\n",
    "\n",
    "        return most_common_emotion\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "        text_prediction = None\n",
    "\n",
    "        # Step 1: Predict using audio model if audio is provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "\n",
    "            # Step 2: Extract text from audio and predict using text model\n",
    "            extracted_text = extract_text_from_audio(audio_path)\n",
    "            if extracted_text:\n",
    "                text_prediction = self.text_model.text_classify(extracted_text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from extracted text: {text_emotion}\")\n",
    "            else:\n",
    "                print(\"No text extracted from audio.\")\n",
    "\n",
    "        # Step 3: Predict from image if image path is provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # If predictions are available, apply fusion logic\n",
    "        if len(predictions) > 0:\n",
    "            final_emotion = self.priority_majority_fusion(predictions, text_prediction)\n",
    "            print(f\"Final predicted emotion: {final_emotion}\")\n",
    "            return final_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5385553-1eca-4865-a947-eb90cb0355c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61482722-0e5b-4a5f-aa4e-74dbb7968572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "Final predicted emotion: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "# Test with all data present (text, image, and audio)\n",
    "image_input_path = \"./Image/6 Emotions for image classification/anger.jpg/gv232bwvvkb8m27mh1.jpg\"\n",
    "audio_input_path = \"./AUDIO/TESS Toronto emotional speech set data/OAF_angry/OAF_burn_angry.wav\"\n",
    "text_input = extract_text_from_audio(audio_input_path)\n",
    "\n",
    "final_prediction = multi_modal_model.predict(text=text_input, image_path=image_input_path, audio_path=audio_input_path)\n",
    "\n",
    "# Test with missing data (for example, only text and audio)\n",
    "final_prediction = multi_modal_model.predict(text=text_input, audio_path=audio_input_path)\n",
    "\n",
    "# Test with only one modality (image)\n",
    "final_prediction = multi_modal_model.predict(image_path=image_input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5d992-1d78-416c-a830-9d09585a914e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86ae03bf-7921-42dc-8fc2-228bbf20bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674ms/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "final_prediction = multi_modal_model.predict(text=text_input, image_path=image_input_path, audio_path=audio_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6d7dd-0288-4ef5-9e58-ff9977496129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ced54-5ff2-4706-a8c1-e2d71ff1f1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175420aa-18b7-49a8-bd84-6d10a591e12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5e37e-95d5-4fa5-8d55-e526ddd93144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94372a79-1564-4f30-bb7f-d9d78c98d78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1621a157-deac-4fa6-8d16-b2e4f317bd7a",
   "metadata": {},
   "source": [
    "# _________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cee5d619-d514-4fa2-a619-c75658a7b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018B5ECB3060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018B5EAE7060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "Final predicted emotion: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "from collections import Counter\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def priority_majority_fusion(self, predictions, text_prediction=None):\n",
    "        # Priority-majority fusion logic:\n",
    "\n",
    "        # If all predictions are different, prioritize image only if all are unequal and text is \"Neutral\"\n",
    "        if len(set(predictions)) == 3:\n",
    "            if text_prediction == \"Neutral\":\n",
    "                return \"Neutral\"  # If text is Neutral, return Neutral\n",
    "            return predictions[2]  # Otherwise, return the image prediction if all are different\n",
    "        \n",
    "        # Majority voting logic\n",
    "        most_common_emotion, _ = Counter(predictions).most_common(1)[0]\n",
    "        \n",
    "        # If text is \"Neutral\" and the majority is different, return majority, not neutral\n",
    "        if text_prediction == \"Neutral\" and most_common_emotion != \"Neutral\":\n",
    "            return most_common_emotion\n",
    "\n",
    "        return most_common_emotion\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "        text_prediction = None\n",
    "\n",
    "        # Step 1: Try extracting text from audio if audio is provided\n",
    "        if audio_path is not None:\n",
    "            extracted_text = extract_text_from_audio(audio_path)\n",
    "            \n",
    "            if extracted_text:  # If text was successfully extracted from audio\n",
    "                text_prediction = self.text_model.text_classify(extracted_text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from extracted text: {text_emotion}\")\n",
    "            else:\n",
    "                print(\"No text extracted from audio. Proceeding with audio and image prediction.\")\n",
    "\n",
    "                # Step 2: Predict from audio if no text is extracted\n",
    "                try:\n",
    "                    audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                    audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                    predictions.append(audio_emotion)\n",
    "                    print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in audio prediction: {e}\")\n",
    "\n",
    "        # Step 3: Predict from image if image path is provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # If predictions are available, apply fusion logic\n",
    "        if len(predictions) > 0:\n",
    "            final_emotion = self.priority_majority_fusion(predictions, text_prediction)\n",
    "            print(f\"Final predicted emotion: {final_emotion}\")\n",
    "            return final_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with all data present (text, image, and audio)\n",
    "image_input_path = \"./Image/6 Emotions for image classification/anger.jpg/gv232bwvvkb8m27mh1.jpg\"\n",
    "audio_input_path = \"./AUDIO/TESS Toronto emotional speech set data/OAF_angry/OAF_burn_angry.wav\"\n",
    "text_input = extract_text_from_audio(audio_input_path)\n",
    "\n",
    "final_prediction = multi_modal_model.predict(text=text_input, image_path=image_input_path, audio_path=audio_input_path)\n",
    "\n",
    "# Test with missing data (for example, only text and audio)\n",
    "final_prediction = multi_modal_model.predict(text=text_input, audio_path=audio_input_path)\n",
    "\n",
    "# Test with only one modality (image)\n",
    "final_prediction = multi_modal_model.predict(image_path=image_input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8deed86-5298-43a7-b71c-7246e345b92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1a587-de1d-44c2-9f7e-1d32cb01dfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd27ecc7-503a-40dc-9908-f83cc104545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted emotion from audio: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969ms/step\n",
      "Predicted emotion from extracted text: Neutral\n",
      "Final predicted emotion: Anger\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Predicted emotion from image: Anger\n",
      "Final predicted emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "from collections import Counter\n",
    "\n",
    "# Load the pre-trained models\n",
    "with open('audio_classifier_model.pkl', 'rb') as f:\n",
    "    audio_classifier = pickle.load(f)\n",
    "\n",
    "with open('text_classifier_model.pkl', 'rb') as f:\n",
    "    text_classifier = pickle.load(f)\n",
    "\n",
    "with open('image_predictor.pkl', 'rb') as f:\n",
    "    image_predictor = pickle.load(f)\n",
    "\n",
    "# Emotion Labels for different models\n",
    "emotion_labels_img = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\"]\n",
    "emotion_labels_audio = [\"Happiness\", \"Neutral\", \"Sadness\", \"Anger\", \"Fear\", \"Disgust\"]\n",
    "emotion_labels_text = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Function to extract text from audio using SpeechRecognition\n",
    "def extract_text_from_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "        print(f\"Error in audio text extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "class MultiModalSentimentAnalysis:\n",
    "    def __init__(self, audio_model, text_model, image_model):\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "    def priority_majority_fusion(self, predictions, text_prediction=None):\n",
    "        # Priority-majority fusion logic:\n",
    "\n",
    "        # If all predictions are different, prioritize image only if all are unequal and text is \"Neutral\"\n",
    "        if len(set(predictions)) == 3:\n",
    "            if text_prediction == \"Neutral\":\n",
    "                return \"Neutral\"  # If text is Neutral, return Neutral\n",
    "            return predictions[2]  # Otherwise, return the image prediction if all are different\n",
    "        \n",
    "        # Majority voting logic\n",
    "        most_common_emotion, _ = Counter(predictions).most_common(1)[0]\n",
    "        \n",
    "        # If text is \"Neutral\" and the majority is different, return majority, not neutral\n",
    "        if text_prediction == \"Neutral\" and most_common_emotion != \"Neutral\":\n",
    "            return most_common_emotion\n",
    "\n",
    "        return most_common_emotion\n",
    "\n",
    "    def predict(self, text=None, image_path=None, audio_path=None):\n",
    "        predictions = []\n",
    "        text_prediction = None\n",
    "\n",
    "        # Step 1: Predict using audio model if audio is provided\n",
    "        if audio_path is not None:\n",
    "            try:\n",
    "                audio_prediction = self.audio_model.audio_classify(audio_path)\n",
    "                audio_emotion = emotion_labels_audio[int(audio_prediction)]\n",
    "                predictions.append(audio_emotion)\n",
    "                print(f\"Predicted emotion from audio: {audio_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio prediction: {e}\")\n",
    "\n",
    "            # Step 2: Extract text from audio and predict using text model\n",
    "            extracted_text = extract_text_from_audio(audio_path)\n",
    "            if extracted_text:\n",
    "                text_prediction = self.text_model.text_classify(extracted_text)\n",
    "                text_emotion = emotion_labels_text[int(text_prediction)]\n",
    "                predictions.append(text_emotion)\n",
    "                print(f\"Predicted emotion from extracted text: {text_emotion}\")\n",
    "            else:\n",
    "                print(\"No text extracted from audio.\")\n",
    "\n",
    "        # Step 3: Predict from image if image path is provided\n",
    "        if image_path is not None:\n",
    "            try:\n",
    "                image_prediction = self.image_model.image_classify(image_path)\n",
    "                image_emotion = emotion_labels_img[int(image_prediction)]\n",
    "                predictions.append(image_emotion)\n",
    "                print(f\"Predicted emotion from image: {image_emotion}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in image prediction: {e}\")\n",
    "        \n",
    "        # If predictions are available, apply fusion logic\n",
    "        if len(predictions) > 0:\n",
    "            final_emotion = self.priority_majority_fusion(predictions, text_prediction)\n",
    "            print(f\"Final predicted emotion: {final_emotion}\")\n",
    "            return final_emotion\n",
    "        else:\n",
    "            print(\"No data provided for prediction.\")\n",
    "            return None\n",
    "\n",
    "# Instantiate the multi-modal sentiment analysis model\n",
    "multi_modal_model = MultiModalSentimentAnalysis(\n",
    "    audio_model=audio_classifier,\n",
    "    text_model=text_classifier,\n",
    "    image_model=image_predictor\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Test with all data present (text, image, and audio)\n",
    "image_input_path = \"./Image/6 Emotions for image classification/anger.jpg/gv232bwvvkb8m27mh1.jpg\"\n",
    "audio_input_path = \"./AUDIO/TESS Toronto emotional speech set data/OAF_angry/OAF_burn_angry.wav\"\n",
    "text_input = extract_text_from_audio(audio_input_path)\n",
    "\n",
    "final_prediction = multi_modal_model.predict(text=text_input, image_path=image_input_path, audio_path=audio_input_path)\n",
    "\n",
    "# Test with missing data (for example, only text and audio)\n",
    "final_prediction = multi_modal_model.predict(text=text_input, audio_path=audio_input_path)\n",
    "\n",
    "# Test with only one modality (image)\n",
    "final_prediction = multi_modal_model.predict(image_path=image_input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7e7eb-23d3-4d25-a7c0-e7810b75dd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a5622-a54e-4e60-b70b-8899eb74a027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
