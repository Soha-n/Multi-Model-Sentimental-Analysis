{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab429e6a-509f-4cd1-bb3f-4217161f596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk import word_tokenize\n",
    "from statistics import mode\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "import tensorflow as tf  \n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "973ef2af-663a-4320-b682-087d14a4a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filenames = [\n",
    "    'nltk_2_naive_bayes_model_40.pkl',\n",
    "    'nltk_naive_bayes_model_42.pkl',\n",
    "    'sklearn_BNB_57.pkl',\n",
    "    'sklearn_Lsvc_model_60.pkl',\n",
    "    'sklearn_m_LRC_61.pkl',\n",
    "    'sklearn_m_SGD_61.pkl',\n",
    "    'sklearn_naive_bayes_model_52.pkl',\n",
    "    'sklearn_svc_model_61.pkl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b84ad3f5-74c6-4853-91f6-abac36e2e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "        classifiers.append(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49b4392e-5e29-4b8c-b722-7b925aebfa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load Keras model\n",
    "h5_model = load_model('trained_model_68_.h5')\n",
    "classifiers.append(h5_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bed6108f-8b8a-4777-94f3-2442823f20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bcd0c5-c003-470b-9e8b-6f4855e2579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function for NLTK models\n",
    "def preprocess_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token not in stopwords_set]\n",
    "    return {word: True for word in tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c136bcdf-5124-411d-b808-c89b6a16ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        votes = []\n",
    "\n",
    "        # Transform the input sentence into TF-IDF features\n",
    "        features_tfidf = vectorizer.transform([sentence])  # Ensure this is a 2D array\n",
    "        processed_features = self.preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                # Predict using the scikit-learn classifiers\n",
    "                prediction = clf.predict(features_tfidf)  # No need to index; already returns an array\n",
    "                votes.append(str(prediction[0]))  # Append the prediction\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                # Predict using the NLTK classifiers\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                # Handle the Keras model\n",
    "                cleaned_sentence = self.clean_new_text(sentence)\n",
    "                # Make sure to format input as needed for Keras\n",
    "                prediction = clf.predict(np.array([cleaned_sentence]))  # Adjust this as necessary\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))  # Append the predicted class\n",
    "\n",
    "        # Return the most common vote\n",
    "        return mode(votes)\n",
    "\n",
    "    def preprocess_nltk(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha() and token not in stopwords_set]\n",
    "        return {word: True for word in tokens}\n",
    "\n",
    "    def confidence(self, sentence):\n",
    "        votes = []\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = self.preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "    def clean_new_text(self, text):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        text = text.lower()\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        cleaned_text = \" \".join(words)\n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8658fb9-a3f8-472d-8901-f95e616165fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "voting_classifier = VoteClassifier(*classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71fe1f56-fabf-4af5-ba3d-c7da040d4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 763ms/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n"
     ]
    }
   ],
   "source": [
    "emotion_labels = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "sentence = \"I am feeling really Happy today!\"\n",
    "predicted_emotion = voting_classifier.classify(sentence)\n",
    "confidence = voting_classifier.confidence(sentence)\n",
    "\n",
    "# Convert the predicted emotion index from string to integer\n",
    "predicted_index = int(predicted_emotion)\n",
    "\n",
    "# Access the corresponding emotion label using the index\n",
    "if 0 <= predicted_index < len(emotion_labels):\n",
    "    corresponding_emotion = emotion_labels[predicted_index]\n",
    "    print(f\"Predicted emotion (index {predicted_index}): {corresponding_emotion}\")\n",
    "else:\n",
    "    print(f\"Index {predicted_index} is out of range for emotion_labels.\")\n",
    "\n",
    "print(f\"Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32cc820f-7747-4c7b-8a16-b43fe78d7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('voting_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(voting_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c83b7-894e-417f-ad60-57e2305e644e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee9a717e-4bf7-4f22-99c3-2f8ff87924ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load all models\n",
    "model_filenames = [\n",
    "    'nltk_2_naive_bayes_model_40.pkl',\n",
    "    'nltk_naive_bayes_model_42.pkl',\n",
    "    'sklearn_BNB_57.pkl',\n",
    "    'sklearn_Lsvc_model_60.pkl',\n",
    "    'sklearn_m_LRC_61.pkl',\n",
    "    'sklearn_m_SGD_61.pkl',\n",
    "    'sklearn_naive_bayes_model_52.pkl',\n",
    "    'sklearn_svc_model_61.pkl'\n",
    "]\n",
    "\n",
    "classifiers = []\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "# Load the Keras model\n",
    "h5_model = load_model('trained_model_68_.h5')\n",
    "classifiers.append(h5_model)\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "# Define the VoteClassifier class (as provided)\n",
    "\n",
    "# Create the voting classifier\n",
    "voting_classifier = VoteClassifier(*classifiers)\n",
    "\n",
    "# Save the ensemble model to a .pkl file\n",
    "with open('voting_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(voting_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f7612-aa42-48d0-a8b0-68fb2fc90b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aa163-e886-472f-bf1b-12c3eb7ea4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57619ec4-8ba5-4919-ba37-026e6b92aca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "224631ca-07ab-4677-ba99-502c7f4c3687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load trained models\n",
    "model_filenames = [\n",
    "    'nltk_2_naive_bayes_model_40.pkl',\n",
    "    'nltk_naive_bayes_model_42.pkl',\n",
    "    'sklearn_BNB_57.pkl',\n",
    "    'sklearn_Lsvc_model_60.pkl',\n",
    "    'sklearn_m_LRC_61.pkl',\n",
    "    'sklearn_m_SGD_61.pkl',\n",
    "    'sklearn_naive_bayes_model_52.pkl',\n",
    "    'sklearn_svc_model_61.pkl'\n",
    "]\n",
    "\n",
    "classifiers = []\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "# Load Keras model\n",
    "h5_model = load_model('trained_model_68_.h5')\n",
    "classifiers.append(h5_model)\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "# Define a wrapper class for the voting classifier\n",
    "class VotingEnsemble:\n",
    "    def __init__(self, classifiers, vectorizer):\n",
    "        self.classifiers = classifiers\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenize and clean the input text\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text).lower()\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word not in stopwords_set]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        # Preprocess the input\n",
    "        preprocessed_sentence = self.preprocess_text(sentence)\n",
    "        \n",
    "        # Transform using the vectorizer\n",
    "        features_tfidf = self.vectorizer.transform([preprocessed_sentence])\n",
    "        \n",
    "        votes = []\n",
    "        for clf in self.classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                # Adjust for NLTK classifiers\n",
    "                tokens = {word: True for word in preprocessed_sentence.split()}\n",
    "                prediction = clf.classify(tokens)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                # Handle Keras model\n",
    "                prediction_input = self.vectorizer.transform([preprocessed_sentence]).toarray()\n",
    "                prediction = clf.predict(prediction_input)\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))\n",
    "        \n",
    "        # Return the most common prediction\n",
    "        return mode(votes)\n",
    "\n",
    "# Create the ensemble object\n",
    "voting_ensemble = VotingEnsemble(classifiers, vectorizer)\n",
    "\n",
    "# Save the entire ensemble including preprocessing to a .pkl file\n",
    "with open('ok.pkl', 'wb') as f:\n",
    "    pickle.dump(voting_ensemble, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36bc21b1-62ec-4fb0-93b3-a6e2a4d153fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssttg\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 24 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "with open('voting_ensemble_with_preprocessing.pkl', 'rb') as file:\n",
    "    text_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d1fe9-c5f3-4fa5-8437-c29710b18e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6442a648-6304-46a1-a13f-83f50a8937f8",
   "metadata": {},
   "source": [
    "# _______ NOT USEFULL ______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "683c6ce1-e63e-48dc-85a6-c0ef3db0fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 954ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644ms/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from keras.models import load_model\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# Load your trained models\n",
    "model_filenames = [\n",
    "    'nltk_2_naive_bayes_model_40.pkl',\n",
    "    'nltk_naive_bayes_model_42.pkl',\n",
    "    'sklearn_BNB_57.pkl',\n",
    "    'sklearn_Lsvc_model_60.pkl',\n",
    "    'sklearn_m_LRC_61.pkl',\n",
    "    'sklearn_m_SGD_61.pkl',\n",
    "    'sklearn_naive_bayes_model_52.pkl',\n",
    "    'sklearn_svc_model_61.pkl'\n",
    "]\n",
    "\n",
    "classifiers = []\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "# Load Keras model\n",
    "h5_model = load_model('trained_model_68_.h5')\n",
    "classifiers.append(h5_model)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    \n",
    "# Preprocess function for NLTK models\n",
    "def preprocess_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token not in stopwords_set]\n",
    "    return {word: True for word in tokens}\n",
    "\n",
    "# Voting classifier\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        votes = []\n",
    "\n",
    "        # Transform the input sentence into TF-IDF features\n",
    "        features_tfidf = vectorizer.transform([sentence])  # Ensure this is a 2D array\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                # Predict using the scikit-learn classifiers\n",
    "                prediction = clf.predict(features_tfidf)  # No need to index; already returns an array\n",
    "                votes.append(str(prediction[0]))  # Append the prediction\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                # Predict using the NLTK classifiers\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                # Handle the Keras model\n",
    "                cleaned_sentence = self.clean_new_text(sentence)\n",
    "                # Make sure to format input as needed for Keras\n",
    "                prediction = clf.predict(np.array([cleaned_sentence]))  # Adjust this as necessary\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))  # Append the predicted class\n",
    "\n",
    "        # Return the most common vote\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, sentence):\n",
    "        votes = []\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "    def clean_new_text(self, text):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        text = text.lower()\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        cleaned_text = \" \".join(words)\n",
    "        return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "voting_classifier = VoteClassifier(*classifiers)\n",
    "\n",
    "emotion_labels = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "sentence = \"I am feeling really Happy today!\"\n",
    "predicted_emotion = voting_classifier.classify(sentence)\n",
    "confidence = voting_classifier.confidence(sentence)\n",
    "\n",
    "# Convert the predicted emotion index from string to integer\n",
    "predicted_index = int(predicted_emotion)\n",
    "\n",
    "# Access the corresponding emotion label using the index\n",
    "if 0 <= predicted_index < len(emotion_labels):\n",
    "    corresponding_emotion = emotion_labels[predicted_index]\n",
    "    print(f\"Predicted emotion (index {predicted_index}): {corresponding_emotion}\")\n",
    "else:\n",
    "    print(f\"Index {predicted_index} is out of range for emotion_labels.\")\n",
    "\n",
    "print(f\"Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192e387-5686-46e3-9f01-9d4161edfe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea83383-4044-4348-a057-04d4cadb0cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2735ff-0f47-4936-83e2-2b208c18a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fb44f-383e-4465-8f5a-38680504974f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f7e62c-89a2-4a75-b23e-24b033ffd322",
   "metadata": {},
   "source": [
    "# __________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95fd45c-8158-41bb-9bc0-9e99e58283fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step\n",
      "Predicted emotion: 3\n",
      "Confidence: 0.78\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from keras.models import load_model\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# Load your trained models\n",
    "model_filenames = [\n",
    "    'nltk_2_naive_bayes_model_40.pkl',\n",
    "    'nltk_naive_bayes_model_42.pkl',\n",
    "    'sklearn_BNB_57.pkl',\n",
    "    'sklearn_Lsvc_model_60.pkl',\n",
    "    'sklearn_m_LRC_61.pkl',\n",
    "    'sklearn_m_SGD_61.pkl',\n",
    "    'sklearn_naive_bayes_model_52.pkl',\n",
    "    'sklearn_svc_model_61.pkl'\n",
    "]\n",
    "\n",
    "classifiers = []\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "# Load Keras model\n",
    "h5_model = load_model('trained_model_68_.h5')\n",
    "classifiers.append(h5_model)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    \n",
    "# Preprocess function for NLTK models\n",
    "def preprocess_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token not in stopwords_set]\n",
    "    return {word: True for word in tokens}\n",
    "\n",
    "# Voting classifier\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        votes = []\n",
    "\n",
    "        # Transform the input sentence into TF-IDF features\n",
    "        features_tfidf = vectorizer.transform([sentence])  # Ensure this is a 2D array\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                # Predict using the scikit-learn classifiers\n",
    "                prediction = clf.predict(features_tfidf)  # No need to index; already returns an array\n",
    "                votes.append(str(prediction[0]))  # Append the prediction\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                # Predict using the NLTK classifiers\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                # Handle the Keras model\n",
    "                cleaned_sentence = self.clean_new_text(sentence)\n",
    "                # Make sure to format input as needed for Keras\n",
    "                prediction = clf.predict(np.array([cleaned_sentence]))  # Adjust this as necessary\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))  # Append the predicted class\n",
    "\n",
    "        # Return the most common vote\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, sentence):\n",
    "        votes = []\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "    def clean_new_text(self, text):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        text = text.lower()\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        cleaned_text = \" \".join(words)\n",
    "        return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "voting_classifier = VoteClassifier(*classifiers)\n",
    "\n",
    "sentence = \"I am feeling really happy today!\"\n",
    "predicted_emotion = voting_classifier.classify(sentence)\n",
    "confidence = voting_classifier.confidence(sentence)\n",
    "\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819a446-fc09-4ab5-aceb-b821dd77af47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d2fd9ad-8229-4558-ab0c-217f2691591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n"
     ]
    }
   ],
   "source": [
    "# emotion_predictor.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from keras.models import load_model\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        votes = []\n",
    "        features_tfidf = vectorizer.transform([sentence])  # Ensure this is a 2D array\n",
    "        processed_features = self.preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                cleaned_sentence = self.clean_new_text(sentence)\n",
    "                prediction = clf.predict(np.array([cleaned_sentence]))\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))\n",
    "\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, sentence):\n",
    "        votes = []\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = self.preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "    def clean_new_text(self, text):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        text = text.lower()\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        cleaned_text = \" \".join(words)\n",
    "        return cleaned_text\n",
    "\n",
    "    def preprocess_nltk(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha() and token not in stopwords_set]\n",
    "        return {word: True for word in tokens}\n",
    "\n",
    "# Load models and vectorizer\n",
    "def load_models(model_filenames, vectorizer_filename):\n",
    "    classifiers = []\n",
    "    for model_filename in model_filenames:\n",
    "        with open(model_filename, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "            classifiers.append(classifier)\n",
    "\n",
    "    # Load Keras model\n",
    "    h5_model = load_model('trained_model_68_.h5')\n",
    "    classifiers.append(h5_model)\n",
    "\n",
    "    # Load TF-IDF vectorizer\n",
    "    with open(vectorizer_filename, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    \n",
    "    return classifiers, vectorizer\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(sentence):\n",
    "    emotion_labels = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "    \n",
    "    # Load models and vectorizer\n",
    "    model_filenames = [\n",
    "        'nltk_2_naive_bayes_model_40.pkl',\n",
    "        'nltk_naive_bayes_model_42.pkl',\n",
    "        'sklearn_BNB_57.pkl',\n",
    "        'sklearn_Lsvc_model_60.pkl',\n",
    "        'sklearn_m_LRC_61.pkl',\n",
    "        'sklearn_m_SGD_61.pkl',\n",
    "        'sklearn_naive_bayes_model_52.pkl',\n",
    "        'sklearn_svc_model_61.pkl'\n",
    "    ]\n",
    "    \n",
    "    classifiers, vectorizer = load_models(model_filenames, 'tfidf_vectorizer.pkl')\n",
    "    \n",
    "    # Initialize the voting classifier\n",
    "    voting_classifier = VoteClassifier(*classifiers)\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_emotion = voting_classifier.classify(sentence)\n",
    "    confidence = voting_classifier.confidence(sentence)\n",
    "\n",
    "    # Convert the predicted emotion index from string to integer\n",
    "    predicted_index = int(predicted_emotion)\n",
    "\n",
    "    # Access the corresponding emotion label using the index\n",
    "    if 0 <= predicted_index < len(emotion_labels):\n",
    "        corresponding_emotion = emotion_labels[predicted_index]\n",
    "        print(f\"Predicted emotion (index {predicted_index}): {corresponding_emotion}\")\n",
    "    else:\n",
    "        print(f\"Index {predicted_index} is out of range for emotion_labels.\")\n",
    "\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sentence = \"I am feeling really happy today!\"\n",
    "    predict_emotion(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "133e6ed5-4367-4fa7-865b-9031ac801992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 4): Sadness\n",
      "Confidence: 0.56\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 0): Anger\n",
      "Confidence: 0.78\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:802 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[0] = 0 is not in [0, 0)\n\t [[{{node RaggedGather/RaggedGather}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m predict_emotion(sample_sentence)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Evaluate accuracy using the validation dataset\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m evaluate_accuracy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAL_DATA.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[52], line 146\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[1;34m(val_data_path)\u001b[0m\n\u001b[0;32m    144\u001b[0m sentence \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    145\u001b[0m true_emotion \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 146\u001b[0m predicted_emotion, _ \u001b[38;5;241m=\u001b[39m predict_emotion(sentence)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Check if the predicted emotion matches the true emotion\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_emotion \u001b[38;5;241m==\u001b[39m true_emotion:\n",
      "Cell \u001b[1;32mIn[52], line 121\u001b[0m, in \u001b[0;36mpredict_emotion\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_emotion\u001b[39m(sentence):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     predicted_emotion \u001b[38;5;241m=\u001b[39m voting_classifier\u001b[38;5;241m.\u001b[39mclassify(sentence)\n\u001b[0;32m    122\u001b[0m     confidence \u001b[38;5;241m=\u001b[39m voting_classifier\u001b[38;5;241m.\u001b[39mconfidence(sentence)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# Convert the predicted emotion index from string to integer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 67\u001b[0m, in \u001b[0;36mVoteClassifier.classify\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifiers:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(clf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 67\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(features_tfidf)\n\u001b[0;32m     68\u001b[0m         votes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(prediction[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(clf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:802 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[0] = 0 is not in [0, 0)\n\t [[{{node RaggedGather/RaggedGather}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI\n",
    "from keras.models import load_model\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Set stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# Load your trained models\n",
    "model_filenames = [\n",
    "    'nltk_2_naive_bayes_model_40.pkl',\n",
    "    'nltk_naive_bayes_model_42.pkl',\n",
    "    'sklearn_BNB_57.pkl',\n",
    "    'sklearn_Lsvc_model_60.pkl',\n",
    "    'sklearn_m_LRC_61.pkl',\n",
    "    'sklearn_m_SGD_61.pkl',\n",
    "    'sklearn_naive_bayes_model_52.pkl',\n",
    "    'sklearn_svc_model_61.pkl'\n",
    "]\n",
    "\n",
    "classifiers = []\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "# Load Keras model\n",
    "h5_model = load_model('trained_model_68_.h5')\n",
    "classifiers.append(h5_model)\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    tokenizer = vectorizer\n",
    "\n",
    "\n",
    "# Maximum length used during training (adjust this as needed)\n",
    "max_length = 100\n",
    "\n",
    "# Preprocess function for NLTK models\n",
    "def preprocess_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token not in stopwords_set]\n",
    "    return {word: True for word in tokens}\n",
    "\n",
    "# Voting classifier\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        votes = []\n",
    "\n",
    "        # Transform the input sentence into TF-IDF features\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                # Handle the Keras model\n",
    "                cleaned_sentence = self.clean_new_text(sentence)\n",
    "                prediction_input = self.prepare_keras_input(cleaned_sentence)\n",
    "                prediction = clf.predict(prediction_input)\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))\n",
    "\n",
    "        # Return the most common vote\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, sentence):\n",
    "        votes = []\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "    def clean_new_text(self, text):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        text = text.lower()\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        cleaned_text = \" \".join(words)\n",
    "        return cleaned_text\n",
    "\n",
    "    def prepare_keras_input(self, text):\n",
    "        sequences = tokenizer.texts_to_sequences([text])\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "        return padded_sequences\n",
    "\n",
    "# Example usage\n",
    "voting_classifier = VoteClassifier(*classifiers)\n",
    "\n",
    "emotion_labels = [\"Anger\", \"Disgust\", \"Fear\", \"Happiness\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "def predict_emotion(sentence):\n",
    "    # Make prediction\n",
    "    predicted_emotion = voting_classifier.classify(sentence)\n",
    "    confidence = voting_classifier.confidence(sentence)\n",
    "\n",
    "    # Convert the predicted emotion index from string to integer\n",
    "    predicted_index = int(predicted_emotion)\n",
    "\n",
    "    # Access the corresponding emotion label using the index\n",
    "    if 0 <= predicted_index < len(emotion_labels):\n",
    "        corresponding_emotion = emotion_labels[predicted_index]\n",
    "        print(f\"Predicted emotion (index {predicted_index}): {corresponding_emotion}\")\n",
    "    else:\n",
    "        print(f\"Index {predicted_index} is out of range for emotion_labels.\")\n",
    "\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "\n",
    "    return corresponding_emotion, confidence\n",
    "\n",
    "def evaluate_accuracy(val_data_path):\n",
    "    val_data = pd.read_csv(val_data_path)\n",
    "    total = len(val_data)\n",
    "    correct = 0\n",
    "\n",
    "    for _, row in val_data.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        true_emotion = row['Emotion']\n",
    "        predicted_emotion, _ = predict_emotion(sentence)\n",
    "\n",
    "        # Check if the predicted emotion matches the true emotion\n",
    "        if predicted_emotion == true_emotion:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Example to predict an emotion and evaluate accuracy\n",
    "if __name__ == \"__main__\":\n",
    "    sample_sentence = \"I am feeling really happy today!\"\n",
    "    predict_emotion(sample_sentence)\n",
    "    \n",
    "    # Evaluate accuracy using the validation dataset\n",
    "    evaluate_accuracy('VAL_DATA.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d7dee29-92d3-49ea-9519-7b91557afd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.33\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.44\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.89\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 4): Sadness\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.89\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 2): Fear\n",
      "Confidence: 0.56\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.44\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 4): Sadness\n",
      "Confidence: 0.44\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 3): Happiness\n",
      "Confidence: 0.78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 0): Anger\n",
      "Confidence: 0.67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 4): Sadness\n",
      "Confidence: 0.44\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted emotion (index 6): Neutral\n",
      "Confidence: 0.67\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:5122 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[0] = 0 is not in [0, 0)\n\t [[{{node RaggedGather_1/RaggedGather}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m predict_emotion(sample_sentence)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Evaluate accuracy using the validation dataset\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m evaluate_accuracy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST_DATA.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[89], line 48\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[1;34m(val_data_path)\u001b[0m\n\u001b[0;32m     46\u001b[0m sentence \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     47\u001b[0m true_emotion \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 48\u001b[0m predicted_emotion, _ \u001b[38;5;241m=\u001b[39m predict_emotion(sentence)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Check if the predicted emotion matches the true emotion\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_emotion \u001b[38;5;241m==\u001b[39m true_emotion:\n",
      "Cell \u001b[1;32mIn[52], line 121\u001b[0m, in \u001b[0;36mpredict_emotion\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_emotion\u001b[39m(sentence):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     predicted_emotion \u001b[38;5;241m=\u001b[39m voting_classifier\u001b[38;5;241m.\u001b[39mclassify(sentence)\n\u001b[0;32m    122\u001b[0m     confidence \u001b[38;5;241m=\u001b[39m voting_classifier\u001b[38;5;241m.\u001b[39mconfidence(sentence)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# Convert the predicted emotion index from string to integer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 67\u001b[0m, in \u001b[0;36mVoteClassifier.classify\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifiers:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(clf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 67\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(features_tfidf)\n\u001b[0;32m     68\u001b[0m         votes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(prediction[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(clf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:5122 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[0] = 0 is not in [0, 0)\n\t [[{{node RaggedGather_1/RaggedGather}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# Existing imports remain the same\n",
    "\n",
    "# Voting classifier\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        votes = []\n",
    "        \n",
    "        # Transform the input sentence into TF-IDF features\n",
    "        features_tfidf = vectorizer.transform([sentence])\n",
    "        processed_features = preprocess_nltk(sentence)\n",
    "\n",
    "        for clf in self._classifiers:\n",
    "            if hasattr(clf, 'predict'):\n",
    "                prediction = clf.predict(features_tfidf)\n",
    "                votes.append(str(prediction[0]))\n",
    "            elif hasattr(clf, 'classify'):\n",
    "                prediction = clf.classify(processed_features)\n",
    "                votes.append(str(prediction))\n",
    "            else:\n",
    "                # Handle the Keras model\n",
    "                cleaned_sentence = self.clean_new_text(sentence)\n",
    "                prediction_input = self.prepare_keras_input(cleaned_sentence)\n",
    "                prediction = clf.predict(prediction_input)\n",
    "                \n",
    "                # Handle prediction for Keras\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                votes.append(str(predicted_class))\n",
    "\n",
    "        # Return the most common vote\n",
    "        return mode(votes)\n",
    "\n",
    "    def prepare_keras_input(self, text):\n",
    "        sequences = tokenizer.texts_to_sequences([text])  # Convert text to sequences\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=max_length)  # Pad sequences\n",
    "        return padded_sequences  # Ensure this is a 2D array\n",
    "\n",
    "def evaluate_accuracy(val_data_path):\n",
    "    val_data = pd.read_csv(val_data_path)\n",
    "    total = len(val_data)\n",
    "    correct = 0\n",
    "\n",
    "    for _, row in val_data.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        true_emotion = row['Emotion']\n",
    "        predicted_emotion, _ = predict_emotion(sentence)\n",
    "\n",
    "        # Check if the predicted emotion matches the true emotion\n",
    "        if predicted_emotion == true_emotion:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Example to predict an emotion and evaluate accuracy\n",
    "if __name__ == \"__main__\":\n",
    "    c=0\n",
    "    sample_sentence = \"I am feeling really happy today!\"\n",
    "    predict_emotion(sample_sentence)\n",
    "    \n",
    "    # Evaluate accuracy using the validation dataset\n",
    "    \n",
    "    evaluate_accuracy('TEST_DATA.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7d47d2d4-5b04-4400-8ab2-be85d6cf34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('TEST_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b2ef3c3-6fd9-4db4-8dfe-700aaa183f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>5</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>3</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Emotion  Length\n",
       "0  My favourite food is anything I didn't have to...        6      59\n",
       "1  Now if he does off himself, everyone will thin...        6     112\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING        0      30\n",
       "3                        To make her feel threatened        2      27\n",
       "4                             Dirty Southern Wankers        0      22\n",
       "5  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...        5      98\n",
       "6  Yes I heard abt the f bombs! That has to be wh...        3     117\n",
       "7  We need more boards and to create a bit more s...        3      82\n",
       "8  Damn youtube and outrage drama is super lucrat...        3      60\n",
       "9  It might be linked to the trust factor of your...        6      54"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1d527-4bd3-47d5-9e2c-90f494d4f652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
